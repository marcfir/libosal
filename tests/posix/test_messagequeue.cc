#include "gtest/gtest.h"
#include <pthread.h>
#include <vector>

#include "libosal/mq.h"
#include "libosal/osal.h"
#include "test_utils.h"

namespace test_messagequeue {

int verbose = 0;

using testutils::wait_nanoseconds;

/*
  This is a test which tests the messagequeue with
  multiple producers, multiple consumers.

  The concept is as follows: N producer threads
  write a series of pseudo-random values to
  K destinations. These randon values are
  generated by hashing the sequence of natural
  numbers.

  In order to ensure that the sequence is
  in the right order, the counter is locked for
  each destination, by a per-destination mutex.

  These values are used to form messages,
  which are all sent to the same message queue.

  Then, M consumer threads read the messages from the
  queue, and append them to the destination.
  Appending happens by hashing the previous
  value together with the received message
  value.

  To compare, the sender threads computed the same
  sequence of hashing, and when the threads finish,
  the resulting hashs are compared. If the message
  queue works correctly (preserving both content
  and order of messages), the hashes have to match.


*/

namespace multiwriter_multireader {
const uint N_PRODUCERS = 30;
const uint M_CONSUMERS = 20;
const uint K_ENDPOINTS = 10;

const ulong NUM_MESSAGES = 1000 * N_PRODUCERS * M_CONSUMERS;
const ulong NUM_MESSAGES_PER_PRODUCER = NUM_MESSAGES / N_PRODUCERS;
const ulong NUM_MESSAGES_PER_CONSUMER = NUM_MESSAGES / M_CONSUMERS;
const ulong MIN_WAIT_TIME_NS = 1000;
const ulong MAX_WAIT_TIME_NS = 100000;

typedef struct {
  uint32_t counter;
  size_t hash;
  pthread_mutex_t source_mutex;
} source_t;

typedef struct {
  uint32_t counter;
  size_t hash;
  pthread_mutex_t dest_mutex;
} dest_t;

typedef struct {
  uint dest_id;
  uint32_t payload;
} message_t;

typedef struct {
  source_t source[K_ENDPOINTS];
  dest_t dest[K_ENDPOINTS];
  pthread_mutex_t receive_lock;
  osal_mq_t queue;
} shared_t;

typedef struct {
  uint32_t thread_id;
  shared_t *pshared;
} thread_data_t;

// combine two hash values (as in a HMAC,
// just not cryptographically secure). */

size_t gethash(uint32_t const n) { return std::hash<uint32_t>{}(n); }

size_t combine_hash(size_t const oldhash, uint32_t const payload) {
  size_t new_hash = std::hash<uint32_t>{}(payload);
  return (oldhash << 4) ^ new_hash;
}

void *run_producer(void *p_params) {

  shared_t *pshared = ((thread_data_t *)p_params)->pshared;
  uint32_t const thread_id = ((thread_data_t *)p_params)->thread_id;

  osal_retval_t orv;
  int rv;

  message_t msg;
  if (verbose) {
    printf("started: producer # %u\n", thread_id);
  }

  for (ulong i = 0; i < NUM_MESSAGES_PER_PRODUCER; i++) {

    // draw a random message id
    msg.dest_id = rand() % K_ENDPOINTS;
    source_t *source = &pshared->source[msg.dest_id];

    // the lock is needed here for two things:
    //
    // 1. to protect the per-endpoint counter, so that we can check
    // ordering later
    //
    // 2. to protect the ordering of messages in respect to that endpoint
    rv = pthread_mutex_lock(&source->source_mutex);
    EXPECT_EQ(rv, OSAL_OK) << "pthread_mutex_lock()[dest] failed";

    source->counter++;
    msg.payload = 0xFFFFFFFF & gethash(source->counter);
    source->hash = combine_hash(source->hash, msg.payload);

    if (verbose) {
      printf("sending from producer thread_id %u to endpoint %u\n", thread_id,
             msg.dest_id);
    }
    osal_uint32_t const prio = 0;
    orv = osal_mq_send(&pshared->queue, (char *)&msg, sizeof(msg), prio);
    EXPECT_EQ(orv, OSAL_OK) << "osal_mq_send() failed";

    if (verbose) {
      printf("sending from producer thread_id %u to endpoint %u .. OK\n",
             thread_id, msg.dest_id);
    }

    // return dest lock
    rv = pthread_mutex_unlock(&source->source_mutex);
    EXPECT_EQ(rv, OSAL_OK) << "pthread_mutex_unlock()[source] failed";

    // wait a bit to retain some queue capacity
    wait_nanoseconds(rand() % MAX_WAIT_TIME_NS);
  }

  if (verbose) {
    printf("exiting: producer # %u\n", thread_id);
  }
  return nullptr;
}

void *run_consumer(void *p_params) {

  shared_t *pshared = ((thread_data_t *)p_params)->pshared;
  uint32_t const thread_id = ((thread_data_t *)p_params)->thread_id;

  osal_retval_t orv;
  int rv;

  message_t msg;

  if (verbose) {
    printf("started: consumer # %u\n", thread_id);
  }

  for (ulong i = 0; i < NUM_MESSAGES_PER_CONSUMER; i++) {

    if (verbose) {
      printf("consumer thread_id %u : locking\n", thread_id);
    }
    rv = pthread_mutex_lock(&pshared->receive_lock);
    EXPECT_EQ(rv, OSAL_OK) << "pthread_mutex_lock() [mq] failed";
    if (verbose) {
      printf("wait/receive from consumer thread_id %u\n", thread_id);
    }
    osal_uint32_t rprio = 0;
    orv = osal_mq_receive(&pshared->queue, (char *)&msg, sizeof(msg), &rprio);
    EXPECT_EQ(orv, OSAL_OK) << "osal_mq_receive() failed";

    dest_t *dest = &pshared->dest[msg.dest_id];

    if (verbose) {
      printf("received from consumer thread_id %u for endpoint %u\n", thread_id,
             msg.dest_id);
    }

    rv = pthread_mutex_lock(&dest->dest_mutex);
    EXPECT_EQ(rv, OSAL_OK) << "pthread_mutex_lock()[dest] failed";

    dest->counter++;
    dest->hash = combine_hash(dest->hash, msg.payload);

    // return dest lock
    rv = pthread_mutex_unlock(&dest->dest_mutex);
    EXPECT_EQ(rv, OSAL_OK) << "pthread_mutex_unlock()[dest] failed";

    // return mq lock
    rv = pthread_mutex_unlock(&pshared->receive_lock);
    EXPECT_EQ(rv, OSAL_OK) << "pthread_mutex_unlock()[mq] failed";
  }

  if (verbose) {
    printf("exiting: consumer # %u\n", thread_id);
  }
  return nullptr;
}

TEST(MessageQueue, MultiSendMultiReceive) {

  int rv;
  osal_retval_t orv;

  shared_t shared;

  pthread_t producers[N_PRODUCERS];
  thread_data_t prod_data[N_PRODUCERS];
  pthread_t consumers[M_CONSUMERS];
  thread_data_t cons_data[M_CONSUMERS];

  // initialize sources
  for (uint i = 0; i < K_ENDPOINTS; i++) {
    shared.source[i].counter = 0;
    shared.source[i].hash = 0;
    rv = pthread_mutex_init(&shared.source[i].source_mutex, nullptr);
    ASSERT_EQ(rv, 0) << "pthread_mutex_init()[source] failed";
  }

  // initialize destinations
  for (uint i = 0; i < K_ENDPOINTS; i++) {
    shared.dest[i].counter = 0;
    shared.dest[i].hash = 0;
    rv = pthread_mutex_init(&shared.dest[i].dest_mutex, nullptr);
    ASSERT_EQ(rv, 0) << "pthread_mutex_init()[dest] failed";
  }

  rv = pthread_mutex_init(&shared.receive_lock, nullptr);
  ASSERT_EQ(rv, 0) << "pthread_mutex_init()[rlock] failed";

  // initialize message queue
  osal_mq_attr_t attr = {};
  attr.oflags = OSAL_MQ_ATTR__OFLAG__RDWR | OSAL_MQ_ATTR__OFLAG__CREAT;
  attr.max_messages = 10; /* system default, won't work with larger
                           * number without adjustment */
  ASSERT_GE(attr.max_messages, 0u);
  attr.max_message_size = sizeof(message_t);
  ASSERT_GE(attr.max_message_size, 0u);
  attr.mode = S_IRUSR | S_IWUSR;
  // unlink message queue if it exists.
  // Note: the return value is intentionally not checked.
  mq_unlink("/test1");

  orv = osal_mq_open(&shared.queue, "/test1", &attr);
  if (orv != 0) {
    perror("failed to open mq:");
  }
  ASSERT_EQ(orv, OSAL_OK) << "osal_mq_open() failed";

  // initialize consumers
  if (verbose) {
    printf("starting consumers\n");
  }
  for (uint i = 0; i < M_CONSUMERS; i++) {
    cons_data[i].pshared = &shared;
    cons_data[i].thread_id = i;
    rv = pthread_create(/*thread*/ &(consumers[i]),
                        /*pthread_attr*/ nullptr,
                        /* start_routine */ run_consumer,
                        /* arg */ (void *)(&cons_data[i]));
    ASSERT_EQ(rv, 0) << "pthread_create()[consumers] failed";
  }

  // initialize producers
  if (verbose) {
    printf("starting producers\n");
  }
  for (uint i = 0; i < N_PRODUCERS; i++) {
    prod_data[i].pshared = &shared;
    prod_data[i].thread_id = i;

    rv = pthread_create(/*thread*/ &(producers[i]),
                        /*pthread_attr*/ nullptr,
                        /* start_routine */ run_producer,
                        /* arg */ (void *)(&prod_data[i]));
    ASSERT_EQ(rv, 0) << "pthread_create()[producers] failed";
  }

  if (verbose) {
    printf("joining producers\n");
  }
  // the following waits for all producers to finish first
  // join producers
  for (uint i = 0; i < N_PRODUCERS; i++) {
    rv = pthread_join(/*thread*/ producers[i],
                      /*retval*/ nullptr);
    ASSERT_EQ(rv, 0) << "pthread_join()[producers] failed";
  }

  // join consumers
  if (verbose) {
    printf("joining consumers\n");
  }
  for (uint i = 0; i < M_CONSUMERS; i++) {
    rv = pthread_join(/*thread*/ consumers[i],
                      /*retval*/ nullptr);
    ASSERT_EQ(rv, 0) << "pthread_join()[consumers] failed";
  }

  // destroy message queue
  orv = osal_mq_close(&shared.queue);
  ASSERT_EQ(orv, OSAL_OK) << "osal_mq_close() failed";

  rv = pthread_mutex_destroy(&shared.receive_lock);
  ASSERT_EQ(rv, 0) << "pthread_mutex_destroy()[rlock] failed";

  // destroy destinations mutexes
  for (uint i = 0; i < K_ENDPOINTS; i++) {
    rv = pthread_mutex_destroy(&shared.dest[i].dest_mutex);
    ASSERT_EQ(rv, 0) << "pthread_mutex_destroy[dest]() failed";
  }

  // destroy sources mutexes
  for (uint i = 0; i < K_ENDPOINTS; i++) {
    rv = pthread_mutex_destroy(&shared.source[i].source_mutex);
    ASSERT_EQ(rv, 0) << "pthread_mutex_destroy[source]() failed";
  }

  // compare results for correctness

  for (ulong i = 0; i < K_ENDPOINTS; i++) {
    EXPECT_EQ(shared.source[i].counter, shared.dest[i].counter)
        << "counters do not match";
  }
  if (verbose) {
    for (ulong i = 0; i < K_ENDPOINTS; i++) {
      printf("hashed values: source = 0x%zx - dest = 0x%zx\n",
             shared.source[i].hash, shared.dest[i].hash);
    }
  }
  for (ulong i = 0; i < K_ENDPOINTS; i++) {
    EXPECT_EQ(shared.source[i].hash, shared.dest[i].hash)
        << "hashes do not match";
  }
}
} // namespace multiwriter_multireader

namespace readonly_writeonly {
const uint N_PRODUCERS = 30;
const uint M_CONSUMERS = 20;
const uint K_ENDPOINTS = 10;

const ulong NUM_MESSAGES = 1000 * N_PRODUCERS * M_CONSUMERS;
const ulong NUM_MESSAGES_PER_PRODUCER = NUM_MESSAGES / N_PRODUCERS;
const ulong NUM_MESSAGES_PER_CONSUMER = NUM_MESSAGES / M_CONSUMERS;
const ulong MIN_WAIT_TIME_NS = 1000;
const ulong MAX_WAIT_TIME_NS = 100000;

typedef struct {
  uint32_t counter;
  size_t hash;
  pthread_mutex_t source_mutex;
} source_t;

typedef struct {
  uint32_t counter;
  size_t hash;
  pthread_mutex_t dest_mutex;
} dest_t;

typedef struct {
  uint dest_id;
  uint32_t payload;
} message_t;

typedef struct {
  source_t source[K_ENDPOINTS];
  dest_t dest[K_ENDPOINTS];
  pthread_mutex_t receive_lock;
  osal_mq_t wqueue;
  osal_mq_t rqueue;
} shared_t;

typedef struct {
  uint32_t thread_id;
  shared_t *pshared;
} thread_data_t;

// combine two hash values (as in a HMAC,
// just not cryptographically secure). */

size_t gethash(uint32_t const n) { return std::hash<uint32_t>{}(n); }

size_t combine_hash(size_t const oldhash, uint32_t const payload) {
  size_t new_hash = std::hash<uint32_t>{}(payload);
  return (oldhash << 4) ^ new_hash;
}

void *run_wproducer(void *p_params) {

  shared_t *pshared = ((thread_data_t *)p_params)->pshared;
  uint32_t const thread_id = ((thread_data_t *)p_params)->thread_id;

  osal_retval_t orv;
  int rv;

  message_t msg;
  if (verbose) {
    printf("started: producer # %u\n", thread_id);
  }

  for (ulong i = 0; i < NUM_MESSAGES_PER_PRODUCER; i++) {

    // draw a random message id
    msg.dest_id = rand() % K_ENDPOINTS;
    source_t *source = &pshared->source[msg.dest_id];

    // the lock is needed here for two things:
    //
    // 1. to protect the per-endpoint counter, so that we can check
    // ordering later
    //
    // 2. to protect the ordering of messages in respect to that endpoint
    rv = pthread_mutex_lock(&source->source_mutex);
    EXPECT_EQ(rv, OSAL_OK) << "pthread_mutex_lock()[dest] failed";

    source->counter++;
    msg.payload = 0xFFFFFFFF & gethash(source->counter);
    source->hash = combine_hash(source->hash, msg.payload);

    if (verbose) {
      printf("sending from producer thread_id %u to endpoint %u\n", thread_id,
             msg.dest_id);
    }
    osal_uint32_t const prio = 0;
    orv = osal_mq_send(&pshared->wqueue, (char *)&msg, sizeof(msg), prio);
    EXPECT_EQ(orv, OSAL_OK) << "osal_mq_send() failed";

    if (verbose) {
      printf("sending from producer thread_id %u to endpoint %u .. OK\n",
             thread_id, msg.dest_id);
    }

    // return dest lock
    rv = pthread_mutex_unlock(&source->source_mutex);
    EXPECT_EQ(rv, OSAL_OK) << "pthread_mutex_unlock()[source] failed";

    // wait a bit to retain some queue capacity
    wait_nanoseconds(rand() % MAX_WAIT_TIME_NS);
  }

  if (verbose) {
    printf("exiting: producer # %u\n", thread_id);
  }
  return nullptr;
}

void *run_rconsumer(void *p_params) {

  shared_t *pshared = ((thread_data_t *)p_params)->pshared;
  uint32_t const thread_id = ((thread_data_t *)p_params)->thread_id;

  osal_retval_t orv;
  int rv;

  message_t msg;

  if (verbose) {
    printf("started: consumer # %u\n", thread_id);
  }

  for (ulong i = 0; i < NUM_MESSAGES_PER_CONSUMER; i++) {

    if (verbose) {
      printf("consumer thread_id %u : locking\n", thread_id);
    }
    rv = pthread_mutex_lock(&pshared->receive_lock);
    EXPECT_EQ(rv, OSAL_OK) << "pthread_mutex_lock() [mq] failed";
    if (verbose) {
      printf("wait/receive from consumer thread_id %u\n", thread_id);
    }
    osal_uint32_t rprio = 0;
    orv = osal_mq_receive(&pshared->rqueue, (char *)&msg, sizeof(msg), &rprio);
    EXPECT_EQ(orv, OSAL_OK) << "osal_mq_receive() failed";

    dest_t *dest = &pshared->dest[msg.dest_id];

    if (verbose) {
      printf("received from consumer thread_id %u for endpoint %u\n", thread_id,
             msg.dest_id);
    }

    rv = pthread_mutex_lock(&dest->dest_mutex);
    EXPECT_EQ(rv, OSAL_OK) << "pthread_mutex_lock()[dest] failed";

    dest->counter++;
    dest->hash = combine_hash(dest->hash, msg.payload);

    // return dest lock
    rv = pthread_mutex_unlock(&dest->dest_mutex);
    EXPECT_EQ(rv, OSAL_OK) << "pthread_mutex_unlock()[dest] failed";

    // return mq lock
    rv = pthread_mutex_unlock(&pshared->receive_lock);
    EXPECT_EQ(rv, OSAL_OK) << "pthread_mutex_unlock()[mq] failed";
  }

  if (verbose) {
    printf("exiting: consumer # %u\n", thread_id);
  }
  return nullptr;
}

TEST(MessageQueue, ReadonlyWriteonly) {

  int rv;
  osal_retval_t orv;

  shared_t shared;

  pthread_t producers[N_PRODUCERS];
  thread_data_t prod_data[N_PRODUCERS];
  pthread_t consumers[M_CONSUMERS];
  thread_data_t cons_data[M_CONSUMERS];

  // initialize sources
  for (uint i = 0; i < K_ENDPOINTS; i++) {
    shared.source[i].counter = 0;
    shared.source[i].hash = 0;
    rv = pthread_mutex_init(&shared.source[i].source_mutex, nullptr);
    ASSERT_EQ(rv, 0) << "pthread_mutex_init()[source] failed";
  }

  // initialize destinations
  for (uint i = 0; i < K_ENDPOINTS; i++) {
    shared.dest[i].counter = 0;
    shared.dest[i].hash = 0;
    rv = pthread_mutex_init(&shared.dest[i].dest_mutex, nullptr);
    ASSERT_EQ(rv, 0) << "pthread_mutex_init()[dest] failed";
  }

  rv = pthread_mutex_init(&shared.receive_lock, nullptr);
  ASSERT_EQ(rv, 0) << "pthread_mutex_init()[rlock] failed";

  // initialize message queue
  osal_mq_attr_t attr_w = {};
  attr_w.oflags = OSAL_MQ_ATTR__OFLAG__WRONLY | OSAL_MQ_ATTR__OFLAG__CREAT;
  attr_w.max_messages = 10; /* system default, won't work with larger
                             * number without adjustment */
  ASSERT_GE(attr_w.max_messages, 0u);
  attr_w.max_message_size = sizeof(message_t);
  ASSERT_GE(attr_w.max_message_size, 0u);
  attr_w.mode = S_IRUSR | S_IWUSR;
  // unlink message queue if it exists.
  // Note: the return value is intentionally not checked.
  mq_unlink("/test2");

  orv = osal_mq_open(&shared.wqueue, "/test2", &attr_w);
  if (orv != 0) {
    perror("failed to open mq:");
  }
  ASSERT_EQ(orv, OSAL_OK) << "osal_mq_open() failed";

  osal_mq_attr_t attr_r = {};
  attr_r.oflags = OSAL_MQ_ATTR__OFLAG__RDONLY;
  attr_r.max_messages = 10; /* system default, won't work with larger
                             * number without adjustment */
  ASSERT_GE(attr_r.max_messages, 0u);
  attr_r.max_message_size = sizeof(message_t);
  ASSERT_GE(attr_r.max_message_size, 0u);
  attr_r.mode = S_IRUSR | S_IWUSR;

  orv = osal_mq_open(&shared.rqueue, "/test2", &attr_r);
  if (orv != 0) {
    perror("failed to open mq:");
  }
  ASSERT_EQ(orv, OSAL_OK) << "osal_mq_open() failed";

  // initialize consumers
  if (verbose) {
    printf("starting consumers\n");
  }
  for (uint i = 0; i < M_CONSUMERS; i++) {
    cons_data[i].pshared = &shared;
    cons_data[i].thread_id = i;
    rv = pthread_create(/*thread*/ &(consumers[i]),
                        /*pthread_attr*/ nullptr,
                        /* start_routine */ run_rconsumer,
                        /* arg */ (void *)(&cons_data[i]));
    ASSERT_EQ(rv, 0) << "pthread_create()[consumers] failed";
  }

  // initialize producers
  if (verbose) {
    printf("starting producers\n");
  }
  for (uint i = 0; i < N_PRODUCERS; i++) {
    prod_data[i].pshared = &shared;
    prod_data[i].thread_id = i;

    rv = pthread_create(/*thread*/ &(producers[i]),
                        /*pthread_attr*/ nullptr,
                        /* start_routine */ run_wproducer,
                        /* arg */ (void *)(&prod_data[i]));
    ASSERT_EQ(rv, 0) << "pthread_create()[producers] failed";
  }

  if (verbose) {
    printf("joining producers\n");
  }
  // the following waits for all producers to finish first
  // join producers
  for (uint i = 0; i < N_PRODUCERS; i++) {
    rv = pthread_join(/*thread*/ producers[i],
                      /*retval*/ nullptr);
    ASSERT_EQ(rv, 0) << "pthread_join()[producers] failed";
  }

  // join consumers
  if (verbose) {
    printf("joining consumers\n");
  }
  for (uint i = 0; i < M_CONSUMERS; i++) {
    rv = pthread_join(/*thread*/ consumers[i],
                      /*retval*/ nullptr);
    ASSERT_EQ(rv, 0) << "pthread_join()[consumers] failed";
  }

  // destroy message queues
  orv = osal_mq_close(&shared.rqueue);
  ASSERT_EQ(orv, OSAL_OK) << "osal_mq_close() failed";

  orv = osal_mq_close(&shared.wqueue);
  ASSERT_EQ(orv, OSAL_OK) << "osal_mq_close() failed";

  rv = pthread_mutex_destroy(&shared.receive_lock);
  ASSERT_EQ(rv, 0) << "pthread_mutex_destroy()[rlock] failed";

  // destroy destinations mutexes
  for (uint i = 0; i < K_ENDPOINTS; i++) {
    rv = pthread_mutex_destroy(&shared.dest[i].dest_mutex);
    ASSERT_EQ(rv, 0) << "pthread_mutex_destroy[dest]() failed";
  }

  // destroy sources mutexes
  for (uint i = 0; i < K_ENDPOINTS; i++) {
    rv = pthread_mutex_destroy(&shared.source[i].source_mutex);
    ASSERT_EQ(rv, 0) << "pthread_mutex_destroy[source]() failed";
  }

  // compare results for correctness

  for (ulong i = 0; i < K_ENDPOINTS; i++) {
    EXPECT_EQ(shared.source[i].counter, shared.dest[i].counter)
        << "counters do not match";
  }
  if (verbose) {
    for (ulong i = 0; i < K_ENDPOINTS; i++) {
      printf("hashed values: source = 0x%zx - dest = 0x%zx\n",
             shared.source[i].hash, shared.dest[i].hash);
    }
  }
  for (ulong i = 0; i < K_ENDPOINTS; i++) {
    EXPECT_EQ(shared.source[i].hash, shared.dest[i].hash)
        << "hashes do not match";
  }
}
} // namespace readonly_writeonly

namespace test_invalidparams {
TEST(MessageQueue, InvalidParamsAccess) {

  int rv;
  osal_retval_t orv;
  osal_mq_t fqueue;
  osal_mq_t gqueue;

  // initialize message queue
  osal_mq_attr_t attr = {};
  attr.oflags = OSAL_MQ_ATTR__OFLAG__WRONLY | OSAL_MQ_ATTR__OFLAG__CREAT;
  attr.max_messages = 10; /* system default, won't work with larger
                           * number without adjustment */
  ASSERT_GE(attr.max_messages, 0u);
  attr.max_message_size = 256;
  ASSERT_GE(attr.max_message_size, 0u);
  attr.mode = S_IRUSR | S_IWUSR;
  // unlink message queue if it exists.
  // Note: the return value is intentionally not checked.
  mq_unlink("/test3");

  orv = osal_mq_open(&fqueue, "/test3", &attr);
  if (orv != 0) {
    perror("failed to open mq:");
  }
  ASSERT_EQ(orv, OSAL_OK) << "osal_mq_open() failed";

  // destroy message queues
  orv = osal_mq_close(&fqueue);
  ASSERT_EQ(orv, OSAL_OK) << "osal_mq_close() failed";

  rv = chmod("/dev/mqueue/test3", S_IROTH);
  ASSERT_EQ(rv, 0) << "chmod() failed";

  attr.oflags = OSAL_MQ_ATTR__OFLAG__WRONLY;
  orv = osal_mq_open(&fqueue, "/test3", &attr);
  if (orv != OSAL_OK) {
    perror("correctly failed to fail opening mq:");
  }
  ASSERT_EQ(orv, OSAL_ERR_PERMISSION_DENIED)
      << "osal_mq_open() succeeded wrongly";

  rv = chmod("/dev/mqueue/test3", S_IRUSR | S_IWUSR);
  ASSERT_EQ(rv, 0) << "chmod() failed";
  attr.oflags = (OSAL_MQ_ATTR__OFLAG__RDWR | OSAL_MQ_ATTR__OFLAG__CREAT |
                 OSAL_MQ_ATTR__OFLAG__EXCL);

  mq_unlink("/test4");

  orv = osal_mq_open(&fqueue, "/test4", &attr);
  if (orv != OSAL_OK) {
    perror("failed to open mq /test4:");
  }
  ASSERT_EQ(orv, OSAL_OK) << "osal_mq_open() failed";

  orv = osal_mq_open(&gqueue, "/test4", &attr);
  if (orv == OSAL_OK) {
    perror("failed to check O_EXCL when opening mq /test4:");
  }

  ASSERT_EQ(orv, OSAL_ERR_PERMISSION_DENIED)
      << "osal_mq_open() succeeded wrongly";
}

TEST(MessageQueue, InvalidParamValues) {

  osal_retval_t orv;
  osal_mq_t fqueue;

  // initialize message queue
  osal_mq_attr_t attr = {};
  attr.oflags = OSAL_MQ_ATTR__OFLAG__WRONLY | OSAL_MQ_ATTR__OFLAG__CREAT;
  attr.max_messages = 10; /* system default, won't work with larger
                           * number without adjustment */
  ASSERT_GE(attr.max_messages, 0u);
  attr.max_message_size = 1 << 31;
  ASSERT_GE(attr.max_message_size, 0u);
  attr.mode = S_IRUSR | S_IWUSR;
  // unlink message queue if it exists.
  // Note: the return value is intentionally not checked.
  mq_unlink("/test5");

  orv = osal_mq_open(&fqueue, "/test5", &attr);
  if (orv != 0) {
    perror("failed to open mq:");
  }
  ASSERT_EQ(orv, OSAL_ERR_INVALID_PARAM)
      << "osal_mq_open() failed to check invalid message size";
}

TEST(MessageQueue, NonExistingName) {

  osal_retval_t orv;
  osal_mq_t fqueue;

  // initialize message queue
  osal_mq_attr_t attr = {};
  attr.oflags = OSAL_MQ_ATTR__OFLAG__WRONLY;
  attr.max_messages = 10; /* system default, won't work with larger
                           * number without adjustment */
  ASSERT_GE(attr.max_messages, 0u);
  attr.max_message_size = 256;
  ASSERT_GE(attr.max_message_size, 0u);
  attr.mode = S_IRUSR | S_IWUSR;
  // unlink message queue if it exists.
  // Note: the return value is intentionally not checked.
  mq_unlink("/test6");

  orv = osal_mq_open(&fqueue, "/test6", &attr);
  if (orv != 0) {
    perror("failed to open mq:");
  }
  ASSERT_EQ(orv, OSAL_ERR_NOT_FOUND)
      << "osal_mq_open() failed to check non-existant mq name";
}

TEST(MessageQueue, OverlyLongName) {

  osal_retval_t orv;
  osal_mq_t fqueue;

  // initialize message queue
  osal_mq_attr_t attr = {};
  attr.oflags = OSAL_MQ_ATTR__OFLAG__WRONLY | OSAL_MQ_ATTR__OFLAG__CREAT;
  attr.max_messages = 10; /* system default, won't work with larger
                           * number without adjustment */
  ASSERT_GE(attr.max_messages, 0u);
  attr.max_message_size = 256;
  ASSERT_GE(attr.max_message_size, 0u);
  attr.mode = S_IRUSR | S_IWUSR;
  // unlink message queue if it exists.
  // Note: the return value is intentionally not checked.
  const size_t name_len = 10000;
  char queue_name[name_len];
  queue_name[0] = '/';
  for (size_t i = 1; i < (name_len - 1); i++) {
    queue_name[i] = 'a';
  }
  queue_name[name_len - 1] = '\0';

  mq_unlink(queue_name);
  errno = 0;
  orv = osal_mq_open(&fqueue, queue_name, &attr);
  if (orv != 0) {
    perror("failed to open mq:");
  }
  ASSERT_EQ(orv, OSAL_ERR_INVALID_PARAM)
      << "osal_mq_open() failed to check overly long mq name";
}

} // namespace test_invalidparams

} // namespace test_messagequeue

int main(int argc, char **argv) {
  ::testing::InitGoogleTest(&argc, argv);
  if (getenv("VERBOSE")) {
    test_messagequeue::verbose = 1;
  }

  return RUN_ALL_TESTS();
}
